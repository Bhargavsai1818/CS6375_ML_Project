{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34829138-f921-4e7c-8252-23e0d560a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text samples from Project Gutenberg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [00:13<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 15 text samples from Project Gutenberg\n",
      "Collecting text samples from Wikipedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████▊                        | 9/20 [00:10<00:12,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 text samples from Wikipedia\n",
      "\n",
      "Total collected: 25 human-written text samples\n",
      "\n",
      "Human text collection complete!\n",
      "Text files are stored in: data/human_texts\n",
      "\n",
      "Sample of collected texts:\n",
      "\n",
      "wikipedia_artificial_intelligence_5432.txt:\n",
      "On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. Th...\n",
      "\n",
      "wikipedia_solar_system_5898.txt:\n",
      "As of the 2020s, a few astronomers have hypothesized that Planet Nine a planet beyond Neptune might exist, based on statistical variance in the orbit ...\n",
      "\n",
      "wikipedia_industrial_revolution_1806.txt:\n",
      "In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement, an important advance in the buildi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HumanTextCollector:\n",
    "    \"\"\"A class to collect human-written texts from various sources.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"data/human_texts\"):\n",
    "        \"\"\"Initialize the collector with an output directory.\"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+', '', text)\n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,?!;:\\'\\\"-]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def save_text(self, text, source, filename=None):\n",
    "        \"\"\"Save text to a file.\"\"\"\n",
    "        if not filename:\n",
    "            # Create a filename based on source and random number\n",
    "            filename = f\"{source.replace(' ', '_').lower()}_{random.randint(1000, 9999)}.txt\"\n",
    "        \n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        # Save the text\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        return file_path\n",
    "    \n",
    "    def fetch_gutenberg_text(self, book_id, min_length=500, max_length=2000):\n",
    "        \"\"\"Fetch text from Project Gutenberg.\"\"\"\n",
    "        try:\n",
    "            url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                # Try alternative URL format\n",
    "                url = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "                response = requests.get(url)\n",
    "                \n",
    "            if response.status_code == 200:\n",
    "                # Get the full text\n",
    "                full_text = response.text\n",
    "                \n",
    "                # Remove Project Gutenberg header and footer\n",
    "                start_marker = \"*** START OF\"\n",
    "                end_marker = \"*** END OF\"\n",
    "                \n",
    "                if start_marker in full_text and end_marker in full_text:\n",
    "                    content = full_text.split(start_marker)[1].split(end_marker)[0]\n",
    "                else:\n",
    "                    content = full_text\n",
    "                \n",
    "                # Split into paragraphs\n",
    "                paragraphs = [p for p in content.split('\\n\\n') if len(p.strip()) > min_length]\n",
    "                \n",
    "                if paragraphs:\n",
    "                    # Select a random paragraph of appropriate length\n",
    "                    selected_texts = []\n",
    "                    for _ in range(3):  # Get 3 samples from this book\n",
    "                        paragraph = random.choice(paragraphs)\n",
    "                        # Truncate if too long\n",
    "                        if len(paragraph) > max_length:\n",
    "                            paragraph = paragraph[:max_length]\n",
    "                        selected_texts.append(self.clean_text(paragraph))\n",
    "                    \n",
    "                    return selected_texts\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Project Gutenberg text {book_id}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def collect_gutenberg_texts(self, book_ids=None, count=15):\n",
    "        \"\"\"Collect texts from Project Gutenberg books.\"\"\"\n",
    "        if not book_ids:\n",
    "            # Popular classic books on Project Gutenberg\n",
    "            book_ids = [\n",
    "                1342,   # Pride and Prejudice\n",
    "                11,     # Alice's Adventures in Wonderland\n",
    "                84,     # Frankenstein\n",
    "                1400,   # Great Expectations\n",
    "                2701,   # Moby Dick\n",
    "                1952,   # The Yellow Wallpaper\n",
    "                74,     # The Adventures of Tom Sawyer\n",
    "                98,     # A Tale of Two Cities\n",
    "                345,    # Dracula\n",
    "                1232,   # The Prince\n",
    "                2600,   # War and Peace\n",
    "                16328,  # Beowulf\n",
    "                76,     # Adventures of Huckleberry Finn\n",
    "                2814,   # Dubliners\n",
    "                1661    # The Adventures of Sherlock Holmes\n",
    "            ]\n",
    "        \n",
    "        saved_files = []\n",
    "        \n",
    "        print(f\"Collecting text samples from Project Gutenberg...\")\n",
    "        for book_id in tqdm(book_ids):\n",
    "            texts = self.fetch_gutenberg_text(book_id)\n",
    "            \n",
    "            for i, text in enumerate(texts):\n",
    "                if text and len(saved_files) < count:\n",
    "                    filename = f\"gutenberg_{book_id}_{i+1}.txt\"\n",
    "                    file_path = self.save_text(text, f\"gutenberg_{book_id}\", filename)\n",
    "                    saved_files.append(file_path)\n",
    "                    \n",
    "                if len(saved_files) >= count:\n",
    "                    break\n",
    "                    \n",
    "        print(f\"Collected {len(saved_files)} text samples from Project Gutenberg\")\n",
    "        return saved_files\n",
    "    \n",
    "    def fetch_wikipedia_text(self, title, min_length=500, max_length=2000):\n",
    "        \"\"\"Fetch text from Wikipedia article.\"\"\"\n",
    "        try:\n",
    "            # Make API request to get the article content\n",
    "            url = \"https://en.wikipedia.org/w/api.php\"\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"titles\": title,\n",
    "                \"prop\": \"extracts\",\n",
    "                \"explaintext\": True\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract the page content\n",
    "            pages = data[\"query\"][\"pages\"]\n",
    "            page_id = next(iter(pages))\n",
    "            \n",
    "            if \"extract\" in pages[page_id]:\n",
    "                extract = pages[page_id][\"extract\"]\n",
    "                \n",
    "                # Split into paragraphs\n",
    "                paragraphs = [p for p in extract.split('\\n') if len(p.strip()) > min_length]\n",
    "                \n",
    "                if paragraphs:\n",
    "                    # Select a suitable paragraph\n",
    "                    paragraph = random.choice(paragraphs)\n",
    "                    # Truncate if too long\n",
    "                    if len(paragraph) > max_length:\n",
    "                        paragraph = paragraph[:max_length]\n",
    "                    \n",
    "                    return self.clean_text(paragraph)\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Wikipedia article {title}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def collect_wikipedia_texts(self, topics=None, count=15):\n",
    "        \"\"\"Collect texts from Wikipedia articles.\"\"\"\n",
    "        if not topics:\n",
    "            # Various topics from different domains\n",
    "            topics = [\n",
    "                \"Artificial_intelligence\",\n",
    "                \"Climate_change\",\n",
    "                \"Quantum_mechanics\",\n",
    "                \"Renaissance\",\n",
    "                \"World_War_II\",\n",
    "                \"Solar_System\",\n",
    "                \"Evolution\",\n",
    "                \"Democracy\",\n",
    "                \"Industrial_Revolution\",\n",
    "                \"Mathematics\",\n",
    "                \"Psychology\",\n",
    "                \"Internet\",\n",
    "                \"Economics\",\n",
    "                \"Agriculture\",\n",
    "                \"Film\",\n",
    "                \"Literature\",\n",
    "                \"Biology\",\n",
    "                \"Philosophy\",\n",
    "                \"Music_theory\",\n",
    "                \"Genetics\"\n",
    "            ]\n",
    "        \n",
    "        saved_files = []\n",
    "        \n",
    "        print(f\"Collecting text samples from Wikipedia...\")\n",
    "        for topic in tqdm(topics):\n",
    "            text = self.fetch_wikipedia_text(topic)\n",
    "            \n",
    "            if text:\n",
    "                file_path = self.save_text(text, f\"wikipedia_{topic}\")\n",
    "                saved_files.append(file_path)\n",
    "                \n",
    "            if len(saved_files) >= count:\n",
    "                break\n",
    "                \n",
    "            # Be kind to Wikipedia API\n",
    "            time.sleep(1)\n",
    "                    \n",
    "        print(f\"Collected {len(saved_files)} text samples from Wikipedia\")\n",
    "        return saved_files\n",
    "    \n",
    "    def collect_all_texts(self, gutenberg_count=10, wikipedia_count=10):\n",
    "        \"\"\"Collect texts from all sources.\"\"\"\n",
    "        all_files = []\n",
    "        \n",
    "        # Collect from Project Gutenberg\n",
    "        gutenberg_files = self.collect_gutenberg_texts(count=gutenberg_count)\n",
    "        all_files.extend(gutenberg_files)\n",
    "        \n",
    "        # Collect from Wikipedia\n",
    "        wikipedia_files = self.collect_wikipedia_texts(count=wikipedia_count)\n",
    "        all_files.extend(wikipedia_files)\n",
    "        \n",
    "        print(f\"\\nTotal collected: {len(all_files)} human-written text samples\")\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set up the collector\n",
    "    collector = HumanTextCollector()\n",
    "    \n",
    "    # Collect human-written texts from various sources\n",
    "    # Adjust the counts as needed\n",
    "    collected_files = collector.collect_all_texts(\n",
    "        gutenberg_count=15,\n",
    "        wikipedia_count=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHuman text collection complete!\")\n",
    "    print(f\"Text files are stored in: {collector.output_dir}\")\n",
    "    \n",
    "    # Display sample of collected texts\n",
    "    if collected_files:\n",
    "        print(\"\\nSample of collected texts:\")\n",
    "        for file_path in random.sample(collected_files, min(3, len(collected_files))):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(f\"\\n{os.path.basename(file_path)}:\")\n",
    "                print(f\"{content[:150]}...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe8314d-7ea4-47cc-8838-bed1d04bcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading human-written texts...\n",
      "Loaded 105 human texts\n",
      "\n",
      "Step 2: Generating AI texts using free LLMs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating texts with mistral: 100%|██████████████| 5/5 [00:00<00:00,  5.77it/s]\n",
      "Generating texts with mistral: 100%|██████████████| 5/5 [00:00<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 AI texts\n",
      "\n",
      "Step 3: Building the dataset...\n",
      "Dataset created with 115 samples (105 human, 10 AI)\n",
      "\n",
      "Step 4: Generating text regenerations for similarity analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regenerating with mistral: 100%|████████████████| 10/10 [00:24<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Saving the final dataset...\n",
      "Dataset saved to ai_text_detection_dataset.csv\n",
      "\n",
      "Data collection complete! Dataset saved to ai_text_detection_dataset.csv\n",
      "Final dataset contains 115 samples with original texts and regenerations\n",
      "\n",
      "Sample of the dataset:\n",
      "                                                text  \\\n",
      "0  Although all of Germany's stated demands had b...   \n",
      "1  The Renaissance began in times of religious tu...   \n",
      "\n",
      "                            source  label topic regeneration_mistral  \n",
      "0  wikipedia_world_war_ii_6311.txt  human   NaN                  NaN  \n",
      "1   wikipedia_renaissance_2117.txt  human   NaN                  NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "\n",
    "# Available free LLMs to use\n",
    "FREE_LLMS = {\n",
    "    \"mistral\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"falcon\": \"tiiuae/falcon-7b-instruct\",\n",
    "    \"bloomz\": \"bigscience/bloomz-p3\",\n",
    "    \"gpt2\": \"gpt2-xl\",\n",
    "    \"flan-t5\": \"google/flan-t5-xl\"\n",
    "}\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self, api_key, models_dict):\n",
    "        self.api_key = api_key\n",
    "        self.models = models_dict\n",
    "        self.headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "        \n",
    "    def query_model(self, model_name, prompt, max_retries=5, backoff_factor=2):\n",
    "        \"\"\"Query the specified model with error handling and backoff strategy\"\"\"\n",
    "        model_id = self.models.get(model_name)\n",
    "        if not model_id:\n",
    "            raise ValueError(f\"Model {model_name} not found in available models\")\n",
    "        \n",
    "        api_url = f\"{API_URL}{model_id}\"\n",
    "        \n",
    "        # Different payload format based on model type\n",
    "        if \"mistral\" in model_name or \"Mixtral\" in model_id:\n",
    "            payload = {\n",
    "                \"inputs\": f\"<s>[INST] {prompt} [/INST]\",\n",
    "                \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "            }\n",
    "        elif \"falcon\" in model_name:\n",
    "            payload = {\n",
    "                \"inputs\": f\"User: {prompt}\\nAssistant:\",\n",
    "                \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "            }\n",
    "        elif \"bloomz\" in model_id:\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "            }\n",
    "        elif \"flan-t5\" in model_id:\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "            }\n",
    "        else:\n",
    "            # Default format for other models\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "            }\n",
    "        \n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(api_url, headers=self.headers, json=payload)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Successfully got a response\n",
    "                    if isinstance(response.json(), list):\n",
    "                        generated_text = response.json()[0].get(\"generated_text\", \"\")\n",
    "                    else:\n",
    "                        generated_text = response.json().get(\"generated_text\", \"\")\n",
    "                    \n",
    "                    return generated_text.strip()\n",
    "                \n",
    "                elif response.status_code == 503:\n",
    "                    # Model is loading\n",
    "                    response_data = response.json() if response.content else {\"estimated_time\": backoff_factor * (2 ** retry)}\n",
    "                    wait_time = response_data.get(\"estimated_time\", backoff_factor * (2 ** retry))\n",
    "                    print(f\"Model {model_name} is loading. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    # Other error\n",
    "                    print(f\"Error {response.status_code}: {response.text}\")\n",
    "                    # Try with a different error handling approach\n",
    "                    if retry < max_retries - 1:\n",
    "                        wait_time = backoff_factor * (2 ** retry)\n",
    "                        print(f\"Retrying in {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    continue\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "                if retry < max_retries - 1:\n",
    "                    wait_time = backoff_factor * (2 ** retry)\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "        \n",
    "        raise Exception(f\"Failed to get response from {model_name} after {max_retries} retries\")\n",
    "    \n",
    "    def generate_text(self, model_name, prompt_template, topic):\n",
    "        \"\"\"Generate text on a specific topic using the specified model\"\"\"\n",
    "        full_prompt = prompt_template.format(topic=topic)\n",
    "        return self.query_model(model_name, full_prompt)\n",
    "    \n",
    "    def regenerate_text(self, model_name, original_text):\n",
    "        \"\"\"Regenerate a given text using the specified model\"\"\"\n",
    "        prompt = f\"Please rewrite the following text in your own words while preserving the key information and tone:\\n\\n{original_text}\\n\\nRewritten text:\"\n",
    "        return self.query_model(model_name, prompt)\n",
    "\n",
    "\n",
    "class DatasetBuilder:\n",
    "    def __init__(self, collector):\n",
    "        self.collector = collector\n",
    "        self.human_texts = []\n",
    "        self.ai_texts = []\n",
    "        self.dataset = pd.DataFrame()\n",
    "        \n",
    "    def load_human_texts_from_files(self, directory_path):\n",
    "        \"\"\"Load human-written texts from text files in the specified directory\"\"\"\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"Created directory {directory_path}. Please add human text files before proceeding.\")\n",
    "            return\n",
    "            \n",
    "        text_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "        if not text_files:\n",
    "            print(f\"No text files found in {directory_path}. Please add some human text files.\")\n",
    "            return\n",
    "            \n",
    "        for filename in text_files:\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read().strip()\n",
    "                    if len(content.split()) >= 30:  # Ensure the text has at least 30 words\n",
    "                        self.human_texts.append({\n",
    "                            'text': content,\n",
    "                            'source': filename,\n",
    "                            'label': 'human'\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                \n",
    "        print(f\"Loaded {len(self.human_texts)} human texts\")\n",
    "    \n",
    "    def generate_ai_texts(self, topics, models=None, samples_per_topic=2):\n",
    "        \"\"\"Generate AI texts on various topics using available models\"\"\"\n",
    "        if not models:\n",
    "            models = list(self.collector.models.keys())\n",
    "            \n",
    "        prompt_template = \"Write a comprehensive and informative passage about {topic}. Be detailed and thorough.\"\n",
    "        \n",
    "        for model in models:\n",
    "            for topic in tqdm(topics, desc=f\"Generating texts with {model}\"):\n",
    "                for _ in range(samples_per_topic):\n",
    "                    try:\n",
    "                        generated_text = self.collector.generate_text(model, prompt_template, topic)\n",
    "                        if generated_text and len(generated_text.split()) >= 30:\n",
    "                            self.ai_texts.append({\n",
    "                                'text': generated_text,\n",
    "                                'source': model,\n",
    "                                'label': 'ai',\n",
    "                                'topic': topic\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating text with {model} on topic '{topic}': {e}\")\n",
    "        \n",
    "        print(f\"Generated {len(self.ai_texts)} AI texts\")\n",
    "    \n",
    "    def build_dataset(self):\n",
    "        \"\"\"Combine human and AI texts into a balanced dataset\"\"\"\n",
    "        all_texts = self.human_texts + self.ai_texts\n",
    "        random.shuffle(all_texts)\n",
    "        self.dataset = pd.DataFrame(all_texts)\n",
    "        return self.dataset\n",
    "    \n",
    "    def process_regenerations(self, models=None, sample_size=None):\n",
    "        \"\"\"Add regenerations of texts using specified models\"\"\"\n",
    "        if not models:\n",
    "            models = list(self.collector.models.keys())[:2]  # Use first 2 models by default\n",
    "        \n",
    "        if sample_size and sample_size < len(self.dataset):\n",
    "            texts_to_process = self.dataset.sample(sample_size)\n",
    "        else:\n",
    "            texts_to_process = self.dataset\n",
    "        \n",
    "        for model in models:\n",
    "            regen_column = f\"regeneration_{model}\"\n",
    "            texts_to_process[regen_column] = None\n",
    "            \n",
    "            for idx, row in tqdm(texts_to_process.iterrows(), total=len(texts_to_process), desc=f\"Regenerating with {model}\"):\n",
    "                try:\n",
    "                    regeneration = self.collector.regenerate_text(model, row['text'])\n",
    "                    self.dataset.loc[idx, regen_column] = regeneration\n",
    "                except Exception as e:\n",
    "                    print(f\"Error regenerating text at index {idx} with {model}: {e}\")\n",
    "                    self.dataset.loc[idx, regen_column] = \"ERROR\"\n",
    "        \n",
    "        return self.dataset\n",
    "    \n",
    "    def save_dataset(self, output_file=\"ai_text_detection_dataset.csv\"):\n",
    "        \"\"\"Save the dataset to a CSV file\"\"\"\n",
    "        self.dataset.to_csv(output_file, index=False)\n",
    "        print(f\"Dataset saved to {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "\n",
    "# Sample topics for AI text generation\n",
    "TOPICS = [\n",
    "    \"climate change\",\n",
    "    \"artificial intelligence ethics\",\n",
    "    \"space exploration\",\n",
    "    \"renewable energy\",\n",
    "    \"quantum computing\",\n",
    "    \"global economics\",\n",
    "    \"modern literature\",\n",
    "    \"cryptocurrency\",\n",
    "    \"sustainable agriculture\",\n",
    "    \"medical advances\",\n",
    "    \"world history\",\n",
    "    \"digital privacy\",\n",
    "    \"education reform\",\n",
    "    \"cultural diversity\",\n",
    "    \"wildlife conservation\"\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create the data collection directory structure\n",
    "    os.makedirs(\"data/human_texts\", exist_ok=True)\n",
    "    \n",
    "    # Initialize the collector and dataset builder\n",
    "    collector = DataCollector(API_KEY, FREE_LLMS)\n",
    "    builder = DatasetBuilder(collector)\n",
    "    \n",
    "    # Step 1: Load human-written texts\n",
    "    print(\"Step 1: Loading human-written texts...\")\n",
    "    builder.load_human_texts_from_files(\"data/human_texts\")\n",
    "    \n",
    "    # If no human texts found, provide instructions and create sample texts\n",
    "    if not builder.human_texts:\n",
    "        print(\"\\nNo human texts found. Creating sample texts for testing...\")\n",
    "        \n",
    "        # Create a few sample human texts directly\n",
    "        sample_texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog. This is a simple test sentence that contains all the letters in the English alphabet. It's commonly used for font testing and other purposes where you need a standard sample text.\",\n",
    "            \"In the heart of the dense forest, a small stream wound its way between moss-covered rocks and fallen logs. Sunlight filtered through the canopy above, creating dappled patterns on the forest floor. Birds called to one another from the branches overhead.\",\n",
    "            \"The history of computing spans decades, from early mechanical calculators to today's sophisticated electronic devices. The development of transistors in the mid-20th century revolutionized the field, allowing for smaller and more efficient computers.\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(sample_texts):\n",
    "            filepath = os.path.join(\"data/human_texts\", f\"sample_text_{i+1}.txt\")\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            builder.human_texts.append({\n",
    "                'text': text,\n",
    "                'source': f\"sample_{i+1}\",\n",
    "                'label': 'human'\n",
    "            })\n",
    "        \n",
    "        print(f\"Created {len(sample_texts)} sample human texts for testing purposes.\")\n",
    "    \n",
    "    # Step 2: Generate AI texts\n",
    "    print(\"\\nStep 2: Generating AI texts using free LLMs...\")\n",
    "    # Use a subset of models to save API calls\n",
    "    used_models = [\"mistral\", \"mistral\"]  # Using models that are more likely to work without special access\n",
    "    \n",
    "    # Simpler topics to reduce complexity\n",
    "    simple_topics = [\n",
    "        \"the benefits of exercise\",\n",
    "        \"healthy eating habits\",\n",
    "        \"importance of education\",\n",
    "        \"history of computers\",\n",
    "        \"climate and weather\"\n",
    "    ]\n",
    "    \n",
    "    builder.generate_ai_texts(simple_topics, models=used_models, samples_per_topic=1)\n",
    "    \n",
    "    # Step 3: Build the combined dataset\n",
    "    print(\"\\nStep 3: Building the dataset...\")\n",
    "    dataset = builder.build_dataset()\n",
    "    print(f\"Dataset created with {len(dataset)} samples ({sum(dataset['label'] == 'human')} human, {sum(dataset['label'] == 'ai')} AI)\")\n",
    "    \n",
    "    # Step 4: Generate regenerations\n",
    "    print(\"\\nStep 4: Generating text regenerations for similarity analysis...\")\n",
    "    # Process just a sample to save API calls\n",
    "    sample_size = min(len(dataset), 10)  # Process up to 10 samples for regeneration\n",
    "    final_dataset = builder.process_regenerations(models=used_models[:1], sample_size=sample_size)\n",
    "    \n",
    "    # Step 5: Save the dataset\n",
    "    print(\"\\nStep 5: Saving the final dataset...\")\n",
    "    output_file = builder.save_dataset()\n",
    "    \n",
    "    print(f\"\\nData collection complete! Dataset saved to {output_file}\")\n",
    "    print(f\"Final dataset contains {len(final_dataset)} samples with original texts and regenerations\")\n",
    "    \n",
    "    # Display dataset sample\n",
    "    print(\"\\nSample of the dataset:\")\n",
    "    print(dataset.head(2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176eaf34-6522-486a-868c-63dac6ca66e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
